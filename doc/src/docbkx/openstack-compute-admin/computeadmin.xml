<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!-- Some useful entities borrowed from HTML -->
<!ENTITY ndash  "&#x2013;">
<!ENTITY mdash  "&#x2014;">
<!ENTITY hellip "&#x2026;">
<!ENTITY nbsp "&#160;">
<!ENTITY CHECK  '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
<imageobject>
<imagedata fileref="img/Check_mark_23x20_02.svg"
format="SVG" scale="60"/>
</imageobject>
</inlinemediaobject>'>

<!ENTITY ARROW  '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
<imageobject>
<imagedata fileref="img/Arrow_east.svg"
format="SVG" scale="60"/>
</imageobject>
</inlinemediaobject>'>
]>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_system-administration-for-openstack-compute">

    <title>System Administration</title>
    <para>By understanding how the different installed nodes interact with each other you can
        administer the OpenStack Compute installation. OpenStack Compute offers many ways to install
        using multiple servers but the general idea is that you can have multiple compute nodes that
        control the virtual servers and a cloud controller node that contains the remaining Nova services. </para>
    <para>The OpenStack Compute cloud works via the interaction of a series of daemon processes
        named nova-* that reside persistently on the host machine or machines. These binaries can
        all run on the same machine or be spread out on multiple boxes in a large deployment. The
        responsibilities of Services, Managers, and Drivers, can be a bit confusing at first. Here
        is an outline the division of responsibilities to make understanding the system a little bit
        easier. </para>
    <para>Currently, Services are nova-api, nova-objectstore (which can be replaced with Glance, the
        OpenStack Image Service), nova-compute, nova-volume, and nova-network. Managers and Drivers
        are specified by configuration options and loaded using utils.load_object(). Managers are responsible for a
        certain aspect of the system. It is a logical grouping of code relating to a portion of the
        system. In general other components should be using the manager to make changes to the
        components that it is responsible for. </para>
    <para>For example, other components that need to deal with volumes in some way, should do so by
        calling methods on the VolumeManager instead of directly changing fields in the database.
        This allows us to keep all of the code relating to volumes in the same place. </para>
    <itemizedlist>
        <listitem>
            <para>nova-api - The nova-api service receives xml requests and sends them to the rest
                of the system. It is a wsgi app that routes and authenticate requests. It supports
                the EC2 and OpenStack APIs. There is a nova-api.conf file created when you install
                Compute.</para>
        </listitem>
        <listitem>
            <para>nova-objectstore - The nova-objectstore service is an ultra simple file-based
                storage system for images that replicates most of the S3 API. It can be replaced
                with OpenStack Image Service and a simple image manager or use OpenStack Object
                Storage as the virtual machine image storage facility. It must reside on the same
                node as nova-compute.</para>
        </listitem>
        <listitem>
            <para>nova-compute - The nova-compute service is responsible for managing virtual
                machines. It loads a Service object which exposes the public methods on
                ComputeManager via Remote Procedure Call (RPC).</para>
        </listitem>
        <listitem>
            <para>nova-volume - The nova-volume service is responsible for managing attachable block
                storage devices. It loads a Service object which exposes the public methods on
                VolumeManager via RPC.</para>
        </listitem>
        <listitem>
            <para>nova-network - The nova-network service is responsible for managing floating and
                fixed IPs, DHCP, bridging and VLANs. It loads a Service object which exposes the
                public methods on one of the subclasses of NetworkManager. Different networking
                strategies are available to the service by changing the network_manager configuration option to
                FlatManager, FlatDHCPManager, or VlanManager (default is VLAN if no other is
                specified).</para>

        </listitem>
    </itemizedlist>
    
    <section xml:id="understanding-the-compute-service-architecture">

        <title>Understanding the Compute Service Architecture</title>
        <para>These basic categories describe the service architecture and what's going on within the cloud controller.</para>
        <simplesect><title>API Server</title>

            <para>At the heart of the cloud framework is an API Server. This API Server makes command and control of the hypervisor, storage, and networking programmatically available to users in realization of the definition of cloud computing.
            </para>
            <para>The API endpoints are basic http web services which handle authentication, authorization, and basic command and control functions using various API interfaces under the Amazon, Rackspace, and related models. This enables API compatibility with multiple existing tool sets created for interaction with offerings from other vendors. This broad compatibility prevents vendor lock-in.
            </para> </simplesect>
        <simplesect><title>Message Queue</title>
            <para>
            A messaging queue brokers the interaction between compute nodes (processing), volumes (block storage), the networking controllers (software which controls network infrastructure), API endpoints, the scheduler (determines which physical hardware to allocate to a virtual resource), and similar components. Communication to and from the cloud controller is by HTTP requests through multiple API endpoints.</para>

<para>            A typical message passing event begins with the API server receiving a request from a user. The API server authenticates the user and ensures that the user is permitted to issue the subject command. Availability of objects implicated in the request is evaluated and, if available, the request is routed to the queuing engine for the relevant workers. Workers continually listen to the queue based on their role, and occasionally their type hostname. When such listening produces a work request, the worker takes assignment of the task and begins its execution. Upon completion, a response is dispatched to the queue which is received by the API server and relayed to the originating user. Database entries are queried, added, or removed as necessary throughout the process.
</para>
</simplesect>
        <simplesect><title>Compute Worker</title>

            <para>Compute workers manage computing instances on host machines. Through the API, commands are dispatched to compute workers to:</para>

            <itemizedlist>
            <listitem><para>Run instances</para></listitem>
            <listitem><para>Terminate instances</para></listitem>
            <listitem><para>Reboot instances</para></listitem>
            <listitem><para>Attach volumes</para></listitem>
            <listitem><para>Detach volumes</para></listitem>
            <listitem><para>Get console output</para></listitem></itemizedlist>
            </simplesect>
            <simplesect><title>Network Controller</title>

            <para>The Network Controller manages the networking resources on host machines. The API server dispatches commands through the message queue, which are subsequently processed by Network Controllers. Specific operations include:</para>

            <itemizedlist><listitem><para>Allocate fixed IP addresses</para></listitem>
            <listitem><para>Configuring VLANs for projects</para></listitem>
            <listitem><para>Configuring networks for compute nodes</para></listitem></itemizedlist>
            </simplesect>
<simplesect><title>Volume Workers</title>

            <para>Volume Workers interact with iSCSI storage to manage LVM-based instance volumes. Specific functions include:
            </para>
            <itemizedlist>
                <listitem><para>Create volumes</para></listitem>
            <listitem><para>Delete volumes</para></listitem>
            <listitem><para>Establish Compute volumes</para></listitem></itemizedlist>

    <para>Volumes may easily be transferred between instances, but may be attached to only a single instance at a time.</para></simplesect></section>
    <section xml:id="managing-compute-users">
        <title>Managing Compute Users</title>
        <para>Access to the Euca2ools (ec2) API is controlled by an access and secret key. The
            userâ€™s access key needs to be included in the request, and the request must be signed
            with the secret key. Upon receipt of API requests, Compute will verify the signature and
            execute commands on behalf of the user. </para>
        <para>In order to begin using nova, you will need to create a
            user with the Identity Service.  </para>
    </section>
    <section xml:id="managing-the-cloud">

        <title>Managing the Cloud</title><para>There are three main tools that a system administrator will find useful to manage their cloud;
            the nova client, the nova-manage command, and the Euca2ools commands. </para>
        <para>The nova-manage command may only be run by cloud administrators. Both
            novaclient and euca2ools can be used by all users, though specific commands may be
            restricted by Role Based Access Control in the deprecated nova auth system or in the Identity Management service. </para>
            <simplesect><title>Using the nova command-line tool</title>
            <para>Installing the python-novaclient gives you a <code>nova</code> shell command that enables
                Compute API interactions from the command line. You install the client, and then provide
                your username and password, set as environment variables for convenience, and then you
                can have the ability to send commands to your cloud on the command-line.</para>
            <para>To install python-novaclient, download the tarball from
                <link xlink:href="http://pypi.python.org/pypi/python-novaclient/2.6.3#downloads">http://pypi.python.org/pypi/python-novaclient/2.6.3#downloads</link> and then install it in your favorite python environment. </para>
            <screen>
<prompt>$</prompt> <userinput>curl -O http://pypi.python.org/packages/source/p/python-novaclient/python-novaclient-2.6.3.tar.gz</userinput>
<prompt>$</prompt> <userinput>tar -zxvf python-novaclient-2.6.3.tar.gz</userinput>
<prompt>$</prompt> <userinput>cd python-novaclient-2.6.3</userinput>
<prompt>$</prompt> <userinput>sudo python setup.py install</userinput>
    </screen>
            <para>Now that you have installed the python-novaclient, confirm the installation by entering:</para>
<screen>
<prompt>$</prompt> <userinput>nova help</userinput>
</screen>
<programlisting>
usage: nova [--debug] [--os_username OS_USERNAME] [--os_password OS_PASSWORD]
            [--os_tenant_name OS_TENANT_NAME] [--os_auth_url OS_AUTH_URL]
            [--os_region_name OS_REGION_NAME] [--service_type SERVICE_TYPE]
            [--service_name SERVICE_NAME] [--endpoint_type ENDPOINT_TYPE]
            [--version VERSION]
            &lt;subcommand&gt; ...
</programlisting>
            <para>In return, you will get a listing of all the
                commands and parameters for the nova command line
                client. By setting up the required parameters as
                environment variables, you can fly through these
                commands on the command line. You can add
                    <literal>--os_username</literal> on the nova
                command, or set them as environment variables: </para>
            <para>
<screen>
<prompt>$</prompt> <userinput>export OS_USERNAME=joecool</userinput>
<prompt>$</prompt> <userinput>export OS_PASSWORD=coolword</userinput>
<prompt>$</prompt> <userinput>export OS_TENANT_NAME=coolu</userinput>
</screen>
            </para><para>Using the Identity Service, you are supplied with an
                authentication endpoint, which nova recognizes as the
                    <literal>OS_AUTH_URL</literal>. </para>
            <para>
                <screen>
<prompt>$</prompt> <userinput>export OS_AUTH_URL=http://hostname:5000/v2.0</userinput>
<prompt>$</prompt> <userinput>export NOVA_VERSION=1.1</userinput>
</screen>
            </para></simplesect>

        <simplesect><title>Using the nova-manage command</title>
        <para>The nova-manage command may be used to perform many essential functions for
                administration and ongoing maintenance of nova, such as network creation or user
                manipulation.</para>
        <para>The man page for nova-manage has a good explanation for each of its functions, and is
              recommended reading for those starting out. Access it by running:
        </para>
            <screen>
            <prompt>$</prompt> <userinput>man nova-manage</userinput>
            </screen>

        <para>For administrators, the standard pattern for executing a nova-manage command is: </para>
            <screen>
<prompt>$</prompt> <userinput>nova-manage category command <replaceable>[args]</replaceable></userinput>
            </screen>

            <para>For example, to obtain a list of all projects:
                    <command>nova-manage project list</command></para>

            <para>Run without arguments to see a list of available command categories: nova-manage</para>
            <para>You can also run with a category argument such as user to see 
	          a list of all commands in that category: nova-manage service</para>
        </simplesect>        <simplesect><title>Using the euca2ools commands</title>
            <para>For a command-line interface to EC2 API calls, use
                the euca2ools command line tool. It is documented at
                    <link
                    xlink:href="http://open.eucalyptus.com/wiki/Euca2oolsGuide_v1.3"
                    >http://open.eucalyptus.com/wiki/Euca2oolsGuide_v1.3</link></para>
        </simplesect>
    </section>
    <section xml:id="live-migration-usage">

        <title>Using Migration</title>
        <para>Before starting migrations, review the <link linkend="configuring-migrations"
                >Configuring Migrations</link> section.</para>
        <para>Migration provides a scheme to migrate running instances from one OpenStack Compute
            server to another OpenStack Compute server. This feature can be used as described below. </para>

        <itemizedlist>
            <listitem>
               <para>First, look at the running instances, to get the ID of the instance you wish to
                    migrate.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova-list</userinput>
<![CDATA[+--------------------------------------+------+--------+-----------------+
|                  ID                  | Name | Status |Networks         |
+--------------------------------------+------+--------+-----------------+
| d1df1b5a-70c4-4fed-98b7-423362f2c47c | vm1  | ACTIVE | private=a.b.c.d |
| d693db9e-a7cf-45ef-a7c9-b3ecb5f22645 | vm2  | ACTIVE | private=e.f.g.h |
+--------------------------------------+------+--------+-----------------+
                 ]]></programlisting>
               <para>Second, look at information associated with that instance - our example is vm1
from above.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova show d1df1b5a-70c4-4fed-98b7-423362f2c47c</userinput>
<![CDATA[+-------------------------------------+----------------------------------------------------------+
|               Property              |                          Value                           |
+-------------------------------------+----------------------------------------------------------+
...
| OS-EXT-SRV-ATTR:host                | HostB                                                    |
...
| flavor                              | m1.tiny                                                  |
| id                                  | d1df1b5a-70c4-4fed-98b7-423362f2c47c                     |
| name                                | vm1                                                      |
| private network                     | a.b.c.d                                                  |
| status                              | ACTIVE                                                   |
...
+-------------------------------------+----------------------------------------------------------+
                 ]]></programlisting>
                 <para> In this example, vm1 is running on HostB.</para>
            </listitem>
            <listitem>
                <para>Third, select the server to migrate instances to.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova-manage service list</userinput>
<![CDATA[HostA nova-scheduler enabled  :-) None
HostA nova-volume enabled  :-) None
HostA nova-network enabled  :-) None
HostB nova-compute enabled  :-) None
HostC nova-compute enabled  :-) None
                 ]]></programlisting>
                 <para> In this example, HostC can be picked up because nova-compute is running on it.</para>
            </listitem>
            <listitem>
                <para>Third, ensure that HostC has enough resource for migration.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova-manage service describe_resource HostC</userinput>
<![CDATA[HOST             PROJECT     cpu   mem(mb)     hdd
HostC(total)                  16     32232     878
HostC(used_now)               13     21284     442
HostC(used_max)               13     21284     442
HostC            p1            5     10240     150
HostC            p2            5     10240     150
.....
                 ]]></programlisting>
                 <itemizedlist>
                     <listitem>
                         <para><emphasis role="bold">cpu:</emphasis>the nuber of cpu</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">mem(mb):</emphasis>total amount of memory (MB)</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">hdd</emphasis>total amount of NOVA-INST-DIR/instances(GB)</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">1st line shows </emphasis>total amount of resource physical server has.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">2nd line shows </emphasis>current used resource.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">3rd line shows </emphasis>maximum used resource.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">4th line and under</emphasis> is used resource per project.</para>
                     </listitem>
                 </itemizedlist>
            </listitem>
            <listitem>
                <para>Finally, use the <command>nova live-migration</command> command to migrate the
                    instances.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova live-migration bee83dd3-5cc9-47bc-a1bd-6d11186692d0 HostC</userinput>
<![CDATA[Migration of bee83dd3-5cc9-47bc-a1bd-6d11186692d0 initiated.
                 ]]></programlisting>
                 <para>Make sure instances are migrated successfully with <command>nova
                        list</command>. If instances are still running on HostB, check logfiles
                    (src/dest nova-compute and nova-scheduler) to determine why.
                     <note><para>While the nova command is called <command>live-migration</command>, under the default Compute
                            configuration options the instances are suspended before migration. See
                            the <link linkend="configuring-migrations">Configuring Migrations</link>
                            section for more details.</para></note></para>
            </listitem>
        </itemizedlist>


    </section>
    <section xml:id="nova-disaster-recovery-process">
        <title>Nova Disaster Recovery Process</title>
        <para>Sometimes, things just don't go right. An incident is
            never planned, by its definition. </para>
        <para>In this section, we will review managing your cloud
            after a disaster, and how to easily backup the persistent
            storage volumes, which is another approach when you face a
            disaster. Even apart from the disaster scenario, backup
            ARE mandatory. While the Diablo release includes the
            snapshot functions, both the backup procedure and the
            utility do apply to the Cactus release. </para>
        <para>For reference, you cand find a DRP definition here : <link
                xlink:href="http://en.wikipedia.org/wiki/Disaster_Recovery_Plan"
                >http://en.wikipedia.org/wiki/Disaster_Recovery_Plan</link>. </para>
            <simplesect>
                <title>A- The disaster Recovery Process presentation</title>
           <para>A disaster could happen to several components of your
                architecture : a disk crash, a network loss, a power
                cut, etc. In this example, we suppose the following
                setup : <orderedlist>
                    <listitem>
                        <para> A cloud controller (nova-api,
                            nova-objecstore, nova-volume,
                            nova-network) </para>
                    </listitem>
                    <listitem>
                        <para> A compute node (nova-compute) </para>
                    </listitem>
                    <listitem>
                        <para> A Storage Area Network used by
                            nova-volumes (aka SAN) </para>
                    </listitem>
                </orderedlist>The example disaster will be the worst
                one : a power loss. That power loss applies to the
                three components. <emphasis role="italic">Let's see
                    what runs and how it runs before the
                    crash</emphasis> : <itemizedlist>
                    <listitem>
                        <para>From the SAN to the cloud controller, we
                            have an active iscsi session (used for the
                            "nova-volumes" LVM's VG). </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node we also have active iscsi sessions
                            (managed by nova-volume). </para>
                    </listitem>
                    <listitem>
                        <para>For every volume an iscsi session is
                            made (so 14 ebs volumes equals 14
                            sessions). </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, we also have iptables/ ebtables
                            rules which allows the acess from the
                            cloud controller to the running instance.
                        </para>
                    </listitem>
                    <listitem>
                        <para>And at least, from the cloud controller
                            to the compute node ; saved into database,
                            the current state of the instances (in
                            that case "running" ), and their volumes
                            attachment (mountpoint, volume id, volume
                            status, etc..) </para>
                    </listitem>
                </itemizedlist> Now, after the power loss occurs and
                all hardware components restart, the situation is as
                follows: </para>
            <para>
                <itemizedlist>
                    <listitem>
                        <para>From the SAN to the cloud, the ISCSI
                            session no longer exists. </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, the ISCSI sessions no longer exist.
                        </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, the iptables and ebtables are
                            recreated, since, at boot, nova-network
                            reapply the configurations. </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller, instances
                            turn into a shutdown state (because they
                            are no longer running) </para>
                    </listitem>
                    <listitem>
                        <para>Into the database, data was not updated
                            at all, since nova could not have guessed
                            the crash. </para>
                    </listitem>
                </itemizedlist>Before going further, and in order to
                prevent the admin to make fatal mistakes,<emphasis
                    role="bold"> the instances won't be
                    lost</emphasis>, since no "<command role="italic"
                    >destroy</command>" or "<command role="italic"
                    >terminate</command>" command had been invoked, so
                the files for the instances remain on the compute
                node. </para>
            <para>The plan is to perform the following tasks, in that
                exact order. <emphasis role="underline">Any extra step
                    would be dangerous at this stage</emphasis>
                :</para>
            <para>
                <orderedlist>
                    <listitem>
               <para>We need to get the current relation from a volume to its instance, since we
                            will recreate the attachment.</para>
                    </listitem>
                    <listitem>
                        <para>We need to update the database in order to clean the stalled state.
                            (After that, we won't be able to perform the first step). </para>
                    </listitem>
                    <listitem>
                        <para>We need to restart the instances (so go from a "shutdown" to a
                            "running" state). </para>
                    </listitem>
                    <listitem>
                        <para>After the restart, we can reattach the volumes to their respective
                            instances. </para>
                    </listitem>
                    <listitem>
                        <para> That step, which is not a mandatory one, exists in an SSH into the
                            instances in order to reboot them. </para>
                    </listitem>
                </orderedlist>
            </para>
            </simplesect>
        <simplesect>
            <title>B - The Disaster Recovery Process itself</title>
            <para>
                <itemizedlist>
                    <listitem>
                        <para><emphasis role="bold"> Instance to
                                Volume relation </emphasis>
                        </para>
                        <para>We need to get the current relation from
                            a volume to its instance, since we will
                            recreate the attachment : </para>
                        <para>This relation could be figured by
                            running
                                <command>nova volume-list</command>
                        </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Database Update
                            </emphasis>
                        </para>
                        <para>Second, we need to update the database
                            in order to clean the stalled state. Now
                            that we have saved the attachments we need
                            to restore for every volume, the database
                            can be cleaned with the following queries:
                            <programlisting>
<prompt>mysql></prompt> <userinput>use nova;</userinput>
<prompt>mysql></prompt> <userinput>update volumes set mountpoint=NULL;</userinput>
<prompt>mysql></prompt> <userinput>update volumes set status="available" where status &lt;&gt;"error_deleting";</userinput>
<prompt>mysql></prompt> <userinput>update volumes set attach_status="detached";</userinput>
<prompt>mysql></prompt> <userinput>update volumes set instance_id=0;</userinput>
                </programlisting>Now,
                            when running
                                <command>nova volume-list</command>
                            all volumes should be available. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Instances Restart
                            </emphasis>
                        </para>
                        <para>We need to restart the instances. This
                            can be done via a simple
                                <command>nova reboot
                                   <replaceable>$instance</replaceable></command>
                        </para>
                        <para>At that stage, depending on your image,
                            some instances will completely reboot and
                            become reacheable, while others will stop
                            on the "plymouth" stage. </para>
                        <para><emphasis role="bold">DO NOT reboot a
                                second time</emphasis> the ones which
                            are stopped at that stage (<emphasis
                                role="italic">see below, the fourth
                                step</emphasis>). In fact it depends
                            on whether you added an
                                <filename>/etc/fstab</filename> entry
                            for that volume or not. Images built with
                            the <emphasis role="italic"
                                >cloud-init</emphasis> package will
                            remain on a pending state, while others
                            will skip the missing volume and start.
                            (More information is available on <link
                                xlink:href="https://help.ubuntu.com/community/CloudInit"
                                >help.ubuntu.com</link>) But remember
                            that the idea of that stage is only to ask
                            nova to reboot every instance, so the
                            stored state is preserved. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Volume Attachment
                            </emphasis>
                        </para>
                        <para>After the restart, we can reattach the
                            volumes to their respective instances. Now
                            that nova has restored the right status,
                            it is time to performe the attachments via
                            a <command>nova volume-attach</command></para>
                        <para>Here is a simple snippet that uses the
                            file we created :
                            <programlisting>
#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    nova volume-attach $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file
                </programlisting>At
                            that stage, instances which were pending
                            on the boot sequence (<emphasis
                                role="italic">plymouth</emphasis>)
                            will automatically continue their boot,
                            and restart normally, while the ones which
                            booted will see the volume. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> SSH into
                                instances </emphasis>
                        </para>
                        <para>If some services depend on the volume,
                            or if a volume has an entry into fstab, it
                            could be good to simply restart the
                            instance. This restart needs to be made
                            from the instance itself, not via nova.
                            So, we SSH into the instance and perform a
                            reboot :
                            <screen>
<prompt>$</prompt> <userinput>shutdown -r now</userinput>
                             </screen></para>
                    </listitem>
                </itemizedlist>Voila! You successfully recovered your
                cloud after that. </para>
            <para>Here are some suggestions : </para>
            <para><itemizedlist>
                    <listitem>
                        <para> Use the <literal>parameter
                                errors=remount</literal> in the
                                <filename>fstab</filename> file, which
                            will prevent data corruption.</para>
                        <para> The system would lock any write to the
                            disk if it detects an I/O error. This
                            configuration option should be added into
                            the nova-volume server (the one which
                            performs the ISCSI connection to the SAN),
                            but also into the instances'
                                <filename>fstab</filename>
                            file.</para>
                    </listitem>
                    <listitem>
                        <para>Do not add the entry for the SAN's disks
                            to the nova-volume's
                                <filename>fstab</filename> file. </para>
                        <para>Some systems will  hang on that step,
                            which means you could lose access to your
                            cloud-controller. In order to re-run the
                            session manually, you would run the
                            following command before performing the
                            mount:
                            <screen>
<prompt>#</prompt> <userinput>iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</userinput>
 </screen></para>
                    </listitem>
                    <listitem>
                        <para> For your instances, if you have the
                            whole <filename>/home/</filename>
                            directory on the disk, instead of emptying
                            the <filename>/home</filename> directory
                            and map the disk on it, leave a user's
                            directory with the user's bash files and
                            the <filename>authorized_keys</filename>
                            file. </para>
                        <para>This will allow you to connect to the
                            instance, even without the volume
                            attached, if you allow only connections
                            via public keys. </para>
                    </listitem>
            </itemizedlist>
                </para>
        </simplesect>
        <simplesect>
                    <title>C- Scripted DRP</title>
            <para>You can download from <link
                    xlink:href="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh"
                    >here</link> a bash script which performs these
                five steps : </para>
            <para>The "test mode" allows you to perform that whole sequence for only one
                instance.</para>
            <para>To reproduce the power loss,  connect to the compute
                node which runs that same instance and close the iscsi
                session. <emphasis role="underline">Do not dettach the
                    volume via
                    <command>nova volume-detach</command></emphasis>, but
                instead manually close the iscsi session. </para>
            <para>In the following example, the iscsi session is
                number 15 for that instance :
                    <screen>
<prompt>$</prompt> <userinput>iscsiadm -m session -u -r 15</userinput>
                    </screen><emphasis
                    role="bold">Do not forget the flag
                        -<literal>r</literal>; otherwise, you will
                    close ALL sessions</emphasis>.</para>
        </simplesect>
    </section>

</chapter>
